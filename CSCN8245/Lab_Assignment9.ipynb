{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment 9\n",
    "#### Student Name: Amit Ghogre\n",
    "#### ID:8833038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentences = [\n",
    "    \"Python is a versatile programming language.\",\n",
    "    \"JavaScript is widely used for web development.\",\n",
    "    \"Java is known for its platform independence.\",\n",
    "    \"Programming involves writing code to solve problems.\",\n",
    "    \"Data structures are crucial for efficient programming.\",\n",
    "    \"Algorithms are step-by-step instructions for solving problems.\",\n",
    "    \"Version control systems help manage code changes in collaboration.\",\n",
    "    \"Debugging is the process of finding and fixing errors in code.\",\n",
    "    \"Web frameworks simplify the development of web applications.\",\n",
    "    \"Artificial intelligence can be applied in various programming tasks.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sample sentences above. You are required for this assignment to implement four functions **from scratch**. \\\\\\\\\\\\\\\\\\\\\\\\\\\\<br>\n",
    "You are required to preprocess the text and apply the tokenization process as done in assignment 8. (3)\n",
    "***THEN***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'versatile', 'programming', 'language']\n",
      "\n",
      "['javascript', 'widely', 'used', 'web', 'development']\n",
      "\n",
      "['java', 'known', 'platform', 'independence']\n",
      "\n",
      "['programming', 'involves', 'writing', 'code', 'solve', 'problem']\n",
      "\n",
      "['data', 'structure', 'crucial', 'efficient', 'programming']\n",
      "\n",
      "['algorithm', 'stepbystep', 'instruction', 'solving', 'problem']\n",
      "\n",
      "['version', 'control', 'system', 'help', 'manage', 'code', 'change', 'collaboration']\n",
      "\n",
      "['debugging', 'process', 'finding', 'fixing', 'error', 'code']\n",
      "\n",
      "['web', 'framework', 'simplify', 'development', 'web', 'application']\n",
      "\n",
      "['artificial', 'intelligence', 'applied', 'various', 'programming', 'task']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(sentence):\n",
    "    # Remove punctuation \n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(sentence.lower())  \n",
    "    \n",
    "    # Remove stop words and lemmatize\n",
    "    clean_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "\n",
    "clean_sentences = []\n",
    "for sentence in sample_sentences:\n",
    "    clean_sentences.append(clean(sentence))\n",
    "    \n",
    "for sentence in clean_sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 1: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the inverted index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the inverted index\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': [1], 'programming': [1, 4, 5, 10], 'language': [1], 'versatile': [1], 'web': [2, 9], 'widely': [2], 'javascript': [2], 'used': [2], 'development': [2, 9], 'java': [3], 'independence': [3], 'platform': [3], 'known': [3], 'involves': [4], 'writing': [4], 'solve': [4], 'code': [4, 7, 8], 'problem': [4, 6], 'data': [5], 'crucial': [5], 'structure': [5], 'efficient': [5], 'solving': [6], 'instruction': [6], 'stepbystep': [6], 'algorithm': [6], 'change': [7], 'version': [7], 'help': [7], 'system': [7], 'collaboration': [7], 'manage': [7], 'control': [7], 'process': [8], 'fixing': [8], 'finding': [8], 'error': [8], 'debugging': [8], 'simplify': [9], 'application': [9], 'framework': [9], 'intelligence': [10], 'applied': [10], 'various': [10], 'artificial': [10], 'task': [10]}\n"
     ]
    }
   ],
   "source": [
    "def get_inverted_index(list_of_sentence_tokens):\n",
    "    inverted_index = {}\n",
    "    for i, sentence in enumerate(list_of_sentence_tokens, start=1):\n",
    "        for word in set(sentence):  # Using set to get unique words in the sentence\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = [i]  # Create a new list with the current sentence ID\n",
    "            else:\n",
    "                inverted_index[word].append(i)  # Append the sentence ID if not already present\n",
    "    return inverted_index\n",
    "\n",
    "print(get_inverted_index(clean_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 2: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the Positional index that is sufficient to represent the document. Assume that each sentence is a document and the sentence ID starts from 1, and the first token in the list is at position 0. Make sure to consider multiple appearance of the same token. (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    ## TODO: Implement the functionality that will return the positional index\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'python': {1: [0]}, 'versatile': {1: [1]}, 'programming': {1: [2], 4: [0], 5: [4], 10: [4]}, 'language': {1: [3]}, 'javascript': {2: [0]}, 'widely': {2: [1]}, 'used': {2: [2]}, 'web': {2: [3], 9: [0, 4]}, 'development': {2: [4], 9: [3]}, 'java': {3: [0]}, 'known': {3: [1]}, 'platform': {3: [2]}, 'independence': {3: [3]}, 'involves': {4: [1]}, 'writing': {4: [2]}, 'code': {4: [3], 7: [5], 8: [5]}, 'solve': {4: [4]}, 'problem': {4: [5], 6: [4]}, 'data': {5: [0]}, 'structure': {5: [1]}, 'crucial': {5: [2]}, 'efficient': {5: [3]}, 'algorithm': {6: [0]}, 'stepbystep': {6: [1]}, 'instruction': {6: [2]}, 'solving': {6: [3]}, 'version': {7: [0]}, 'control': {7: [1]}, 'system': {7: [2]}, 'help': {7: [3]}, 'manage': {7: [4]}, 'change': {7: [6]}, 'collaboration': {7: [7]}, 'debugging': {8: [0]}, 'process': {8: [1]}, 'finding': {8: [2]}, 'fixing': {8: [3]}, 'error': {8: [4]}, 'framework': {9: [1]}, 'simplify': {9: [2]}, 'application': {9: [5]}, 'artificial': {10: [0]}, 'intelligence': {10: [1]}, 'applied': {10: [2]}, 'various': {10: [3]}, 'task': {10: [5]}}\n"
     ]
    }
   ],
   "source": [
    "def get_positional_index(list_of_sentence_tokens):\n",
    "    positional_index = {}\n",
    "    for i, sentence in enumerate(list_of_sentence_tokens, start=1):\n",
    "        #tokens = sentence.split()  # Split the sentence into tokens\n",
    "        for position, word in enumerate(sentence):\n",
    "            if word not in positional_index:\n",
    "                positional_index[word] = {i: [position]}  # Setting sentence and position in a dictionary\n",
    "            elif i not in positional_index[word]:\n",
    "                positional_index[word][i] = [position]  # New list for this sentence ID\n",
    "            else:\n",
    "                positional_index[word][i].append(position)  # Append the existing list for this sentence ID\n",
    "    return positional_index\n",
    "\n",
    "print(get_positional_index(clean_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 3: Create a method that takes as an input a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences. The method MUST return the TF-IDF Matrix that is sufficient to represent the documents. Assume that each sentence is a document and the sentence ID starts from 1. (7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'python': 0.40235947810852507, 'versatile': 0.40235947810852507, 'programming': 0.17328679513998632, 'language': 0.40235947810852507}, {'javascript': 0.3218875824868201, 'widely': 0.3218875824868201, 'used': 0.3218875824868201, 'web': 0.24079456086518725, 'development': 0.24079456086518725}, {'java': 0.40235947810852507, 'known': 0.40235947810852507, 'platform': 0.40235947810852507, 'independence': 0.40235947810852507}, {'programming': 0.11552453009332421, 'involves': 0.26823965207235, 'writing': 0.26823965207235, 'code': 0.15271512197902584, 'solve': 0.26823965207235, 'problem': 0.20066213405432268}, {'data': 0.3218875824868201, 'structure': 0.3218875824868201, 'crucial': 0.3218875824868201, 'efficient': 0.3218875824868201, 'programming': 0.13862943611198905}, {'algorithm': 0.3218875824868201, 'stepbystep': 0.3218875824868201, 'instruction': 0.3218875824868201, 'solving': 0.3218875824868201, 'problem': 0.24079456086518725}, {'version': 0.20117973905426254, 'control': 0.20117973905426254, 'system': 0.20117973905426254, 'help': 0.20117973905426254, 'manage': 0.20117973905426254, 'code': 0.11453634148426939, 'change': 0.20117973905426254, 'collaboration': 0.20117973905426254}, {'debugging': 0.26823965207235, 'process': 0.26823965207235, 'finding': 0.26823965207235, 'fixing': 0.26823965207235, 'error': 0.26823965207235, 'code': 0.15271512197902584}, {'web': 0.40132426810864535, 'framework': 0.26823965207235, 'simplify': 0.26823965207235, 'development': 0.20066213405432268, 'application': 0.26823965207235}, {'artificial': 0.26823965207235, 'intelligence': 0.26823965207235, 'applied': 0.26823965207235, 'various': 0.26823965207235, 'programming': 0.11552453009332421, 'task': 0.26823965207235}]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_TFIDF_matrix(list_of_sentence_tokens):\n",
    "    unique_words = set(word for sentence in list_of_sentence_tokens for word in sentence)  # Get unique words\n",
    "\n",
    "    num_documents = len(list_of_sentence_tokens)\n",
    "    idf_values = {}\n",
    "    for word in unique_words:\n",
    "        word_in_docs = sum(1 for sentence in list_of_sentence_tokens if word in sentence)  # Number of documents containing the word\n",
    "        idf_values[word] = math.log(num_documents / (1 + word_in_docs))  # Calculate IDF for each word\n",
    "\n",
    "    tf_idf_matrix = []\n",
    "    for sentence in list_of_sentence_tokens:\n",
    "        sentence_tf_idf = {}\n",
    "        for word in sentence:\n",
    "            tf = sentence.count(word) / len(sentence)  # Calculate TF for each word in the sentence\n",
    "            idf = idf_values.get(word, 0)  # Get IDF for the word\n",
    "            sentence_tf_idf[word] = tf * idf  # Calculate TF-IDF for the word in the sentence\n",
    "        tf_idf_matrix.append(sentence_tf_idf)  # Add TF-IDF scores for the sentence to the matrix\n",
    "\n",
    "    return tf_idf_matrix\n",
    "\n",
    "print(get_TFIDF_matrix(clean_sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "#### Part 4: Create a method that takes as an input: (10)\n",
    " - a 2-dimensional list where each of the inner dimensions is a sentence list of tokens, and the outer dimension is the list of the sentences.\n",
    " - A method name: \"tfidf\", \"inverted\"\n",
    " - A Search Query\n",
    " - Return the rank of the sentences based on the given method and a query <br>\n",
    "\n",
    "***Hint: For inverted index we just want documents that have the query word/words, for tfidf you must show the ranking based on highest tfidf score***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 5, 10, 2, 3, 6, 7, 8, 9] [1, 5, 4, 10, 2, 3, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def get_ranked_documents(list_of_sentence_tokens, method_name, search_query):\n",
    "    search_query_words = search_query.lower().split()\n",
    "    rank_list = []\n",
    "\n",
    "    if method_name == \"inverted\":\n",
    "        inverted = get_inverted_index(list_of_sentence_tokens)  # Implementing earlier function\n",
    "        for i, sentence in enumerate(list_of_sentence_tokens, start=1):\n",
    "            match = sum(1 for word in search_query_words if word in inverted and i in inverted[word])\n",
    "            rank_list.append((i, match))\n",
    "\n",
    "    elif method_name == \"tfidf\":\n",
    "        tf_idf_matrix = get_TFIDF_matrix(list_of_sentence_tokens)  # Implementing earlier function\n",
    "        for i, scores in enumerate(tf_idf_matrix, start=1):\n",
    "            tfidf = sum(scores.get(word, 0) for word in search_query_words)\n",
    "            rank_list.append((i, tfidf))\n",
    "\n",
    "    rank_list.sort(key=lambda x: x[1], reverse=True)  # Sorting based on the relevance score\n",
    "\n",
    "    ranked_documents = [index for index, _ in rank_list]\n",
    "    return ranked_documents\n",
    "\n",
    "inverted_ranking = get_ranked_documents(clean_sentences, 'inverted', 'programming')\n",
    "tfidf_ranking = get_ranked_documents(clean_sentences, 'tfidf', 'programming')\n",
    "\n",
    "print(inverted_ranking, tfidf_ranking)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
